Final\_Assignment\_CE17B115
================
Pranav Hari
27 April 2019

1. The goal of this exercise is to estimate how many points Florida is going to score in the upcoming match, given the information provided in this database.
-------------------------------------------------------------------------------------------------------------------------------------------------------------

### 1a. Since we have many variables, and some of them are similar indicators, we would like to determine which variables to include in our regression analysis. Remember that is no clear theory regarding which of these variables can cause an increase in the number of points scored by a team, I would ask you to calculate the correlation coefficients between the dependent variable (Y) and all the other variables. Then, you will choose those variables whose correlation coefficient is higher than 0.20, in absolute value (this is an ad-hoc rule). Show the correlation coefficients.

``` r
flogeorge <- read.csv("C:/Users/prana/OneDrive/Sem 3/MA5750/florida_georgia_final.csv",header = TRUE)
flogeorge[1:73,1:24]
```

    ##     y   x1 x2 x3 x4        x5        x6        x7        x8 x9       x10
    ## 1  14 1946 NA NA 33 0.0000000 10.400000 24.000000 0.4333333  0 0.4000000
    ## 2   6 1947  0 14 34 0.3333333 13.333333 17.000000 0.7843137  1 0.0000000
    ## 3  12 1948  0  6 20 0.5000000 17.666667 21.666667 0.8153846  0 0.4000000
    ## 4  28 1949  0 12  7 0.5000000 21.000000 18.833333 1.1150442  1 0.4000000
    ## 5   0 1950  1 28  6 0.7142857 18.571429 16.285714 1.1403509  0 0.4000000
    ## 6   6 1951  0  0  7 0.5714286 18.857143 11.714286 1.6097561  0 0.5000000
    ## 7  30 1952  0  6  0 0.6000000 29.400000 11.200000 2.6250000  0 0.5000000
    ## 8  21 1953  1 30  7 0.2857143 23.142857 11.857143 1.9518072  0 0.7000000
    ## 9  13 1954  1 21 14 0.5714286 12.571429 14.285714 0.8800000  1 0.3000000
    ## 10 19 1955  0 13 13 0.5000000 13.333333 10.833333 1.2307692  0 0.5000000
    ## 11 28 1956  1 19  0 0.7142857 17.571429  7.142857 2.4600000  1 0.4000000
    ## 12 22 1957  1 28  0 0.6666667 17.333333 12.833333 1.3506494  0 0.6000000
    ## 13  7 1958  1 22  6 0.2000000 11.800000 10.000000 1.1800000  0 0.7000000
    ## 14 10 1959  1  7 21 0.4285714 16.857143  9.142857 1.8437500  0 0.5555556
    ## 15 22 1960  0 10 14 0.7142857 11.857143  7.714286 1.5370370  0 0.5000000
    ## 16 21 1961  1 22 14 0.4285714  7.857143 12.142857 0.6470588  0 0.8000000
    ## 17 23 1962  1 21 15 0.5000000 19.375000 12.750000 1.5196078  1 0.4000000
    ## 18 21 1963  1 23 14 0.5000000 11.000000 12.666667 0.8684211  0 0.5454545
    ## 19  7 1964  1 21 14 0.8333333 22.500000  8.666667 2.5961538  1 0.6666667
    ## 20 14 1965  0  7 10 0.6666667 18.833333 12.166667 1.5479452  0 0.7000000
    ## 21 10 1966  1 14 27 1.0000000 27.333333 11.166667 2.4477612  1 0.7000000
    ## 22 17 1967  0 10 16 0.6666667 21.166667 15.333333 1.3804348  0 0.7777778
    ## 23  0 1968  1 17 51 0.5714286 17.285714 14.857143 1.1634615  0 0.6000000
    ## 24 13 1969  0  0 13 0.8571429 35.714286 21.714286 1.6447368  0 0.6000000
    ## 25 24 1970  0 13 17 0.6250000 20.375000 26.500000 0.7688679  0 0.8000000
    ## 26  7 1971  1 24 49 0.2500000 10.875000 26.125000 0.4162679  0 0.6363636
    ## 27  7 1972  0  7 10 0.5000000 21.166667 16.166667 1.3092784  0 0.3636364
    ## 28 11 1973  0  7 10 0.4285714 12.285714 19.428571 0.6323529  1 0.4545455
    ## 29 16 1974  1 11 17 0.8750000 22.500000 14.875000 1.5126050  1 0.6363636
    ## 30  7 1975  0 16 10 0.8750000 29.000000  9.500000 3.0526316  1 0.7272727
    ## 31 27 1976  0  7 41 0.8571429 29.857143 22.000000 1.3571429  1 0.8181818
    ## 32 22 1977  0 27 17 0.5000000 24.000000 20.666667 1.1612903  0 0.7272727
    ## 33 22 1978  1 22 24 0.4285714 23.857143 17.571429 1.3577236  1 0.5454545
    ## 34 10 1979  0 22 33 0.0000000  7.571429 20.571429 0.3680556  0 0.3636364
    ## 35 21 1980  0 10 26 0.8571429 23.285714 11.000000 2.1168831  1 0.0000000
    ## 36 21 1981  0 21 26 0.6250000 23.625000 12.375000 1.9090909  0 0.6363636
    ## 37  0 1982  0 21 44 0.7142857 28.428571 18.000000 1.5793651  1 0.6363636
    ## 38  9 1983  0  0 10 0.7500000 25.500000 14.875000 1.7142857  0 0.7272727
    ## 39 27 1984  0  9  0 0.7500000 32.750000 17.000000 1.9264706  1 0.7272727
    ## 40  3 1985  1 27 24 0.8750000 28.750000 13.875000 2.0720721  1 0.8181818
    ## 41 31 1986  0  3 19 0.5000000 21.500000 16.375000 1.3129771  1 0.8181818
    ## 42 10 1987  1 31 23 0.6250000 30.250000 11.625000 2.6021505  0 0.5454545
    ## 43  3 1988  0 10 26 0.6250000 26.250000  9.750000 2.6923077  0 0.5454545
    ## 44 10 1989  0  3 17 0.7500000 24.500000 12.375000 1.9797980  0 0.5454545
    ## 45 38 1990  0 10  7 0.8750000 34.000000 13.000000 2.6153846  1 0.6363636
    ## 46 45 1991  1 38 13 0.8750000 33.375000 13.000000 2.5673077  1 0.8181818
    ## 47 26 1992  1 45 24 0.6666667 23.000000 21.166667 1.0866142  1 0.9090909
    ## 48 33 1993  1 26 26 0.8333333 40.000000 20.833333 1.9200000  0 0.6666667
    ## 49 52 1994  1 33 14 0.8333333 47.833333 16.000000 2.9895833  0 0.8333333
    ## 50 52 1995  1 52 17 1.0000000 42.333333 20.500000 2.0650406  1 0.8333333
    ## 51 47 1996  1 52  7 1.0000000 52.285714 13.428571 3.8936170  1 1.0000000
    ## 52 17 1997  1 47 37 0.8571429 41.714286 15.000000 2.7809524  1 0.9166667
    ## 53 38 1998  0 17  7 0.8571429 31.571429 14.000000 2.2551020  1 0.8181818
    ## 54 30 1999  1 38 14 0.8571429 39.428571 21.142857 1.8648649  1 0.8181818
    ## 55 34 2000  1 30 23 0.8571429 42.142857 19.428571 2.1691176  1 0.7500000
    ## 56 24 2001  1 34 10 0.8333333 44.000000 11.333333 3.8823529  0 0.8333333
    ## 57 20 2002  1 24 13 0.6250000 27.875000 21.625000 1.2890173  1 0.8181818
    ## 58 16 2003  1 20 13 0.6250000 33.000000 18.000000 1.8333333  1 0.6666667
    ## 59 24 2004  1 16 31 0.5714286 35.142857 21.571429 1.6291391  0 0.6666667
    ## 60 14 2005  0 24 10 0.7142857 32.140000 11.570000 2.7778738  0 0.5833333
    ## 61 21 2006  1 14 14 0.8571429 28.714286 12.000000 2.3928571  0 0.7272727
    ## 62 30 2007  1 21 42 0.7142857 40.428571 23.285714 1.7361963  1 0.9230769
    ## 63 49 2008  0 30 10 0.8571429 42.000000 11.857143 3.5421687  1 0.7500000
    ## 64 41 2009  1 49 17 1.0000000 35.290000 10.140000 3.4802761  1 0.9230769
    ## 65 34 2010  1 41 31 0.5714286 25.570000 18.710000 1.3666489  0 0.9230769
    ## 66 20 2011  1 34 24 0.5714286 26.860000 18.860000 1.4241782  0 0.5833333
    ## 67  9 2012  0 20 17 1.0000000 30.140000 12.140000 2.4827018  1 0.5384615
    ## 68 20 2013  0  9 23 0.5714286 21.140000 16.280000 1.2985258  0 0.8461538
    ## 69 38 2014  0 20 20 0.5000000 28.670000 25.500000 1.1243137  0 0.3333333
    ## 70 27 2015  1 38  3 0.8571429 31.571429 17.285714 1.8264463  0 0.5384615
    ## 71 24 2016  1 27 10 0.8333333 30.333333 12.000000 2.5277778  1 0.7142857
    ## 72 22 2017  1 24  9 0.5000000 23.666667 23.333333 1.0142857  0 0.6923077
    ## 73 25 2018  1 22 15 0.6670000 25.655500 22.556300 1.0058700  0 0.6825400
    ##    x11 x12 x13       x14       x15 x16 x17 x18  x19  x20  x21  x22   x23
    ## 1   30   3 -27 1.0000000 0.8000000   1   1   0 82.0 64.0 72.8 0.00  1.96
    ## 2   30  25  -5 0.5714286 1.0000000   1   1   0 75.9 59.0 66.4 0.00 11.39
    ## 3   30  13 -17 0.8333333 0.6363636   1   1   0 86.0 70.0 70.0 0.00 10.13
    ## 4   30  30   0 0.4285714 0.9000000   0   0   0 69.1 46.0 55.9 0.00  5.52
    ## 5   30  30   0 0.4285714 0.4545455   0   0   0 72.0 57.9 65.1 0.00  6.90
    ## 6   30  30   0 0.4285714 0.5454545   0   0   0 68.0 53.1 60.3 0.00 11.39
    ## 7   30  30   0 0.8000000 0.5000000   1   0   0 77.0 60.1 67.1 0.00  8.29
    ## 8   30  30   0 0.4285714 0.6363636   0   0   1 60.1 44.1 51.2 0.00 12.54
    ## 9   30  30   0 0.7142857 0.2727273   0   0   0 62.1 46.0 52.9 0.00 10.59
    ## 10  30  30   0 0.5714286 0.6000000   0   0   0 69.1 43.0 54.6 0.00  4.83
    ## 11  13  30  17 0.4285714 0.4000000   1   0   0 66.0 37.0 49.7 0.00  7.60
    ## 12  30  30   0 0.2857143 0.3000000   0   0   0 75.9 55.9 63.7 0.00 11.74
    ## 13  19  30  11 0.3333333 0.3000000   0   0   1 78.1 57.0 65.3 0.00  4.72
    ## 14  30  30   0 0.8571429 0.4000000   1   1   0 75.0 43.0 55.7 0.00 15.54
    ## 15  30  30   0 0.7142857 0.7000000   1   0   1 80.1 54.0 66.7 0.00  3.57
    ## 16  30  30   0 0.4285714 0.6000000   0   0   0 73.9 54.0 60.9 0.00  3.34
    ## 17  30  30   0 0.2857143 0.3000000   0   0   1 63.0 37.9 50.5 0.00  8.75
    ## 18  30  30   0 0.5714286 0.3000000   0   0   0 73.9 55.0 62.0 0.00  1.87
    ## 19  30  30   0 0.5714286 0.4000000   1   1   0 81.0 55.9 67.1 0.00  2.30
    ## 20  30  30   0 0.7142857 0.6000000   1   0   0 77.0 63.0 68.5 0.00  9.44
    ## 21   7  30  23 0.8571429 0.6000000   1   1   1 69.1 43.0 53.0 0.00  9.32
    ## 22  30  30   0 0.7142857 0.9000000   0   1   1 75.9 50.0 61.0 0.00  9.44
    ## 23  30   9 -21 0.7142857 0.7000000   0   1   0 66.0 53.1 57.5 0.00  7.83
    ## 24  13  16   3 0.8571429 0.8000000   0   1   1 75.0 52.0 61.0 0.00  7.25
    ## 25  30  30   0 0.5714286 0.6000000   1   0   0 73.9 45.0 58.4 0.00  6.90
    ## 26  30   7 -23 1.0000000 0.5000000   1   1   0 80.1 59.0 68.0 0.00  6.79
    ## 27  30  30   0 0.6250000 0.9090909   0   0   0 80.1 63.0 69.2 0.00  6.79
    ## 28  30  30   0 0.5000000 0.5454545   1   1   1 84.9 50.9 58.8 0.00 16.92
    ## 29   6  30  24 0.6250000 0.5454545   0   1   1 64.0 53.1 57.6 0.04 10.36
    ## 30  11  30  19 0.7500000 0.5454545   1   1   1 79.0 69.1 72.3 0.00  3.57
    ## 31  10   7  -3 0.8750000 0.8181818   1   1   0 68.0 37.0 50.3 0.00  9.44
    ## 32  30  30   0 0.6250000 0.9090909   1   0   1 83.8 68.0 72.0 0.43  4.26
    ## 33  30  11 -19 0.8750000 0.4545455   1   1   0 79.0 54.9 62.9 0.00  3.91
    ## 34  30  25  -5 0.6250000 0.8181818   0   0   0 82.9 64.9 72.4 0.00  8.29
    ## 35  20   2 -18 1.0000000 0.6363636   1   1   1 75.9 39.9 59.8 0.00  3.80
    ## 36  30   4 -26 0.8750000 1.0000000   1   1   1 72.9 43.0 56.5 0.00  7.48
    ## 37  20   3 -17 1.0000000 0.9090909   1   1   1 63.0 39.9 49.6 0.00 14.61
    ## 38   9   4  -5 0.8750000 1.0000000   1   1   1 79.0 63.9 66.6 0.20  6.33
    ## 39  10   8  -2 0.8750000 0.8181818   1   1   0 75.9 50.0 63.1 0.00  7.83
    ## 40   1  17  16 0.7500000 0.6363636   1   1   0 82.0 51.1 64.7 0.00  4.14
    ## 41  30  19 -11 0.7500000 0.6363636   1   1   0 88.0 66.9 75.7 0.00  2.42
    ## 42  17  10  -7 0.7500000 0.7272727   1   1   1 75.0 48.0 60.6 0.00  8.63
    ## 43   0  19  19 0.7500000 0.7272727   1   1   1 82.0 66.0 73.1 0.12 12.33
    ## 44  20  30  10 0.6250000 0.7272727   1   0   1 84.0 46.9 63.2 0.00  2.99
    ## 45  10  30  20 0.5000000 0.5454545   0   0   0 84.0 57.0 66.8 1.65 14.73
    ## 46   6  23  17 0.7500000 0.5454545   1   1   0 72.0 45.0 47.8 0.39 15.31
    ## 47  20   7 -13 0.8750000 0.7272727   1   0   1 84.0 61.9 72.0 0.00  3.80
    ## 48  10  30  20 0.5000000 0.8181818   1   1   0 79.0 65.8 71.2 1.50  8.29
    ## 49   5  30  25 0.6250000 0.4545455   1   0   1 82.0 63.9 69.3 0.00  8.75
    ## 50   3  30  27 0.6250000 0.5454545   1   1   1 77.0 51.1 59.7 0.71 11.16
    ## 51   1  30  29 0.4285714 0.5454545   0   0   1 86.0 64.0 73.3 0.00 11.97
    ## 52   6  14   8 0.8571429 0.4545455   1   0   1 82.0 66.0 72.7 0.63  9.44
    ## 53   6  11   5 0.8571429 0.8181818   1   1   1 87.1 64.0 74.7 0.00  7.02
    ## 54   5  10   5 0.8571429 0.7272727   1   0   1 79.0 64.0 67.9 0.00 10.59
    ## 55   8  13   5 0.8571429 0.6363636   1   1   1 82.4 55.4 67.9 0.00  1.96
    ## 56   6  15   9 0.7142857 0.6363636   1   1   1 75.9 61.1 59.8 0.00 13.69
    ## 57  30   5 -25 1.0000000 0.7272727   1   1   1 75.9 54.0 63.3 0.00  9.44
    ## 58  23   4 -19 0.8750000 0.9166667   1   1   1 81.0 69.1 74.1 0.00 14.38
    ## 59  30  10 -20 0.8571429 0.9090909   1   1   1 82.4 68.0 74.6 0.01  4.14
    ## 60  16   4 -12 1.0000000 0.8333333   1   1   1 68.0 57.2 63.1 0.00 13.92
    ## 61   9  30  21 0.7500000 0.8333333   1   1   1 79.0 59.0 69.5 1.12 12.43
    ## 62  11  18   7 0.7500000 0.6666667   1   1   1 68.0 62.6 64.9 0.13 11.16
    ## 63   8   6  -2 0.8750000 0.8333333   1   1   1 70.0 57.4 63.0 0.04 10.70
    ## 64   1  30  29 0.5714286 0.7500000   1   1   1 86.0 69.8 77.3 0.00  5.52
    ## 65  30  30   0 0.5000000 0.5833333   1   1   1 81.0 57.9 70.0 0.00  5.64
    ## 66  30  22  -8 0.7142857 0.5000000   1   1   1 73.9 64.0 68.8 1.17 10.24
    ## 67   3  12   9 0.8571429 0.7142857   1   1   1 73.9 66.9 70.9 0.01 20.48
    ## 68  30  30   0 0.5714286 0.8571429   0   0   1 84.9 66.0 71.2 1.64  8.29
    ## 69  30   9 -21 0.8571429 0.6153846   1   1   1 77.0 48.0 57.3 0.00 14.15
    ## 70  11  30  19 0.7142857 0.7692308   1   1   1 84.0 66.0 75.0 0.00  4.00
    ## 71  14  30  16 0.5714286 0.7692308   0   1   1 82.0 72.0 75.0 0.00  8.00
    ## 72  30   3 -27 1.0000000 0.6153846   1   1   1 81.0 62.0 76.0 0.02  8.00
    ## 73  28   8 -20 0.8571430 0.7513800   1   0   1 80.0 60.0 75.0 0.01  7.00

``` r
flogeorge <- flogeorge[1:73,1:24]
flogeorge.lm <- lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12+x13+x14+x15+x16+x17+x18+x19+x20+x21+x22+x23,data=flogeorge)
coef(flogeorge.lm)
```

    ##   (Intercept)            x1            x2            x3            x4 
    ## -438.69647881    0.23867808   -6.26381676    0.30396971   -0.30415825 
    ##            x5            x6            x7            x8            x9 
    ##  -11.34364300    0.93965222   -0.04820427   -3.60244133    3.17538118 
    ##           x10           x11           x12           x13           x14 
    ##    5.34793536    0.10184548    0.22598779            NA   -3.34477570 
    ##           x15           x16           x17           x18           x19 
    ##   -5.95571295    2.47770060   -0.60726472   -4.43487581    0.18460574 
    ##           x20           x21           x22           x23 
    ##   -0.18490998   -0.37850921    1.62317655   -0.50458023

### 1b.Based on the analysis performed in the previous section, write the regression model with the chosen dependent variables and explain the expected sign on each of those coefficients.

``` r
for (i in 1:22){
  print(i)
  print(cor(flogeorge[2:73,1],flogeorge[2:73,i+1]))
}
```

    ## [1] 1
    ## [1] 0.4225025
    ## [1] 2
    ## [1] 0.1511095
    ## [1] 3
    ## [1] 0.4605949
    ## [1] 4
    ## [1] -0.2390108
    ## [1] 5
    ## [1] 0.3587662
    ## [1] 6
    ## [1] 0.5776041
    ## [1] 7
    ## [1] 0.1448213
    ## [1] 8
    ## [1] 0.3640355
    ## [1] 9
    ## [1] 0.2350142
    ## [1] 10
    ## [1] 0.405109
    ## [1] 11
    ## [1] -0.331825
    ## [1] 12
    ## [1] 0.05518699
    ## [1] 13
    ## [1] 0.2947587
    ## [1] 14
    ## [1] -0.06661887
    ## [1] 15
    ## [1] -0.03777075
    ## [1] 16
    ## [1] 0.1642413
    ## [1] 17
    ## [1] -0.03793345
    ## [1] 18
    ## [1] 0.1728645
    ## [1] 19
    ## [1] 0.1907395
    ## [1] 20
    ## [1] 0.07117625
    ## [1] 21
    ## [1] 0.09377035
    ## [1] 22
    ## [1] 0.2088106

### Since we must chose those variables whose correlation coeffiecients are greater than 0.2 in terms of absolute value -

we choose x1,x3,x4,x5,x6,x8,x9,x10,x11,x13,x22

``` r
flogeorge.lm1 <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22,data=flogeorge)
coef(flogeorge.lm1)
```

    ##  (Intercept)           x1           x3           x4           x5 
    ## -148.9562538    0.0732474    0.1940423   -0.2968579   -5.9503813 
    ##           x6           x8           x9          x10          x11 
    ##    0.8564583   -3.2373648    4.4738933    4.9681487    0.3805224 
    ##          x13          x22 
    ##    0.2018753    2.0457120

### we can clearly say that negative coefficients decrease the overall points while positive coefficients increase the overall points given this is a linear model

``` r
summary(flogeorge.lm1)
```

    ## 
    ## Call:
    ## lm(formula = y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + 
    ##     x13 + x22, data = flogeorge)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -18.8948  -5.8278  -0.2221   6.9323  17.6969 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) -148.95625  161.24328  -0.924 0.359292    
    ## x1             0.07325    0.08183   0.895 0.374285    
    ## x3             0.19404    0.11071   1.753 0.084769 .  
    ## x4            -0.29686    0.11394  -2.605 0.011553 *  
    ## x5            -5.95038    9.59900  -0.620 0.537673    
    ## x6             0.85646    0.23761   3.605 0.000637 ***
    ## x8            -3.23736    2.50517  -1.292 0.201217    
    ## x9             4.47389    2.59732   1.723 0.090130 .  
    ## x10            4.96815    7.14617   0.695 0.489603    
    ## x11            0.38052    0.23078   1.649 0.104408    
    ## x13            0.20188    0.14364   1.405 0.165047    
    ## x22            2.04571    3.42861   0.597 0.552980    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 9.375 on 60 degrees of freedom
    ##   (1 observation deleted due to missingness)
    ## Multiple R-squared:  0.5263, Adjusted R-squared:  0.4394 
    ## F-statistic:  6.06 on 11 and 60 DF,  p-value: 1.472e-06

### 1c. Run this regression model and show the results. Explain the effect of each coefficient on the number of points scored by Florida.

``` r
pred <- flogeorge[1:73,2:24]
floor(predict(flogeorge.lm1,newdata = pred))
```

    ##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 
    ## NA  8  8 23 18 13 22 24 15 11 15 20 14  9  7 12 20 12 19 15 14 17  1 20 19 
    ## 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 
    ##  1 18 18 16 21 12 25 21  7  6 12 14 11 26 19 20 17  7 14 30 30 24 31 36 42 
    ## 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 
    ## 47 31 25 34 32 29 29 29 23 20 21 29 31 30 27 27 19 22 23 31 30 24 23

### 1d. Run the joint hypothesis that all the coefficients of this model are equal to zero. Show your results

``` r
flogeorge_h1 <- lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12+x13+x14+x15+x16+x17+x18+x19+x20+x21+x22+x23,data=flogeorge)
flogeorge_h0 <- lm(y ~ 1,data=flogeorge_h1$model)
anova(flogeorge_h0,flogeorge_h1)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ 1
    ## Model 2: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + 
    ##     x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + 
    ##     x22 + x23
    ##   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    
    ## 1     71 11131.8                                  
    ## 2     49  3995.3 22    7136.5 3.9784 2.892e-05 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### Clearly we can see that for any amount of significance, the p value is too small hence we can reject the null hypothesis which says that regression is not significant

### 1e. How many more or less points Florida will score if the average number of points per game (X6) increases by 10?

``` r
flogeorge_1 <- flogeorge
flogeorge_1[,7] = flogeorge[,7] + 10
flogeorge.lm1_1 <- lm(y~x1+x2+x3+x4+x5+x6+x8+x9+x10+x12+x14+x15+x16+x17+x18+x21+x22+x23,data=flogeorge_1)
flogeorge_1[1:73,1] - floor(predict(flogeorge.lm1_1,newdata = pred))
```

    ##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 
    ##  NA  12  18  11  -5   3  17  10   8  10  15  15   2  13  25  17  12  14 
    ##  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36 
    ##  -2   8   9  15  12   6   4  15  -1   4  15  -5  15   4   7   8  21  16 
    ##  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54 
    ##  -6   7   3 -12  17   2   6  -2  16  16  12  10  23  15  14  -3  23   6 
    ##  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72 
    ##   8   3   4   9  15   0  14  11  22  20  13   2   4   7  16   2  10  11 
    ##  73 
    ##  14

### 1f. A friend of yours claims that winning the game before facing Georgia gives Florida an extra touchdown advantage. Test this hypothesis using your regression model (show the results).

### Assuming H0: β9 = 0 and H1: β9 ̸=0

``` r
summary(flogeorge.lm)
```

    ## 
    ## Call:
    ## lm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + 
    ##     x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + 
    ##     x20 + x21 + x22 + x23, data = flogeorge)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -21.8558  -5.2721   0.6989   5.3280  15.2284 
    ## 
    ## Coefficients: (1 not defined because of singularities)
    ##              Estimate Std. Error t value Pr(>|t|)  
    ## (Intercept) -438.6965   200.9195  -2.183   0.0338 *
    ## x1             0.2387     0.1066   2.240   0.0297 *
    ## x2            -6.2638     3.5583  -1.760   0.0846 .
    ## x3             0.3040     0.1432   2.122   0.0389 *
    ## x4            -0.3042     0.1215  -2.503   0.0157 *
    ## x5           -11.3436    10.1620  -1.116   0.2697  
    ## x6             0.9396     0.4170   2.253   0.0288 *
    ## x7            -0.0482     0.6833  -0.071   0.9440  
    ## x8            -3.6024     5.8140  -0.620   0.5384  
    ## x9             3.1754     2.5795   1.231   0.2242  
    ## x10            5.3479     7.2782   0.735   0.4660  
    ## x11            0.1018     0.1787   0.570   0.5714  
    ## x12            0.2260     0.1953   1.157   0.2528  
    ## x13                NA         NA      NA       NA  
    ## x14           -3.3448    11.8993  -0.281   0.7798  
    ## x15           -5.9557     8.4334  -0.706   0.4834  
    ## x16            2.4777     3.4259   0.723   0.4730  
    ## x17           -0.6073     3.2007  -0.190   0.8503  
    ## x18           -4.4349     2.9473  -1.505   0.1388  
    ## x19            0.1846     0.3156   0.585   0.5613  
    ## x20           -0.1849     0.3494  -0.529   0.5991  
    ## x21           -0.3785     0.5399  -0.701   0.4866  
    ## x22            1.6232     3.5657   0.455   0.6510  
    ## x23           -0.5046     0.3536  -1.427   0.1599  
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 9.03 on 49 degrees of freedom
    ##   (1 observation deleted due to missingness)
    ## Multiple R-squared:  0.6411, Adjusted R-squared:  0.4799 
    ## F-statistic: 3.978 on 22 and 49 DF,  p-value: 2.892e-05

### from the above summary table we can see that the pvalue for column x9 - ' Dummy variable equal to 1 if Florida won the game previous to the Georgia game' is *0.2242* which is not very small and hence null hypothesis cannot be rejected i.e x9 cannot be regressionally significant

2. Once we have our basic regression model with the variables we choose based on the correlation coefficient we want to make sure that the functional form is the correct one
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### 2.a We should compare the linear model with the log linear model. Convert to logarithm all the variables that do not have any negative or zero values. Test if the linear models is better than the log-linear model. Show your

results. Which model would you continue to use?

``` r
flogeorge_log <- flogeorge
for (i in 1:73) {
  for (j in 2:23) {
  if(flogeorge[i,j]>0 & !(is.na(flogeorge[i,j])))
    flogeorge_log[i,j] <- log1p(flogeorge[i,j])
  else
    flogeorge_log[i,j] <- flogeorge[i,j]
  }
}  
flogeorge_log$y <- flogeorge[1:73,1]
flogeorge_log
```

    ##     y       x1        x2       x3       x4        x5       x6       x7
    ## 1  14 7.574045        NA       NA 3.526361 0.0000000 2.433613 3.218876
    ## 2   6 7.574558 0.0000000 2.708050 3.555348 0.2876821 2.662588 2.890372
    ## 3  12 7.575072 0.0000000 1.945910 3.044522 0.4054651 2.926739 3.120895
    ## 4  28 7.575585 0.0000000 2.564949 2.079442 0.4054651 3.091042 2.987364
    ## 5   0 7.576097 0.6931472 3.367296 1.945910 0.5389965 2.974071 2.849880
    ## 6   6 7.576610 0.0000000 0.000000 2.079442 0.4519851 2.988564 2.542726
    ## 7  30 7.577122 0.0000000 1.945910 0.000000 0.4700036 3.414443 2.501436
    ## 8  21 7.577634 0.6931472 3.433987 2.079442 0.2513144 3.183989 2.553900
    ## 9  13 7.578145 0.6931472 3.091042 2.708050 0.4519851 2.607967 2.726919
    ## 10 19 7.578657 0.0000000 2.639057 2.639057 0.4054651 2.662588 2.470920
    ## 11 28 7.579168 0.6931472 2.995732 0.000000 0.5389965 2.921624 2.097141
    ## 12 22 7.579679 0.6931472 3.367296 0.000000 0.5108256 2.908721 2.627081
    ## 13  7 7.580189 0.6931472 3.135494 1.945910 0.1823216 2.549445 2.397895
    ## 14 10 7.580700 0.6931472 2.079442 3.091042 0.3566749 2.882404 2.316770
    ## 15 22 7.581210 0.0000000 2.397895 2.708050 0.5389965 2.553900 2.164964
    ## 16 21 7.581720 0.6931472 3.135494 2.708050 0.3566749 2.181224 2.575878
    ## 17 23 7.582229 0.6931472 3.091042 2.772589 0.4054651 3.014309 2.621039
    ## 18 21 7.582738 0.6931472 3.178054 2.708050 0.4054651 2.484907 2.614960
    ## 19  7 7.583248 0.6931472 3.091042 2.708050 0.6061358 3.157000 2.268684
    ## 20 14 7.583756 0.0000000 2.079442 2.397895 0.5108256 2.987364 2.577688
    ## 21 10 7.584265 0.6931472 2.708050 3.332205 0.6931472 3.344039 2.498700
    ## 22 17 7.584773 0.0000000 2.397895 2.833213 0.5108256 3.098590 2.793208
    ## 23  0 7.585281 0.6931472 2.890372 3.951244 0.4519851 2.906120 2.763620
    ## 24 13 7.585789 0.0000000 0.000000 2.639057 0.6190392 3.603166 3.122994
    ## 25 24 7.586296 0.0000000 2.639057 2.890372 0.4855078 3.062222 3.314186
    ## 26  7 7.586804 0.6931472 3.218876 3.912023 0.2231436 2.474435 3.300456
    ## 27  7 7.587311 0.0000000 2.079442 2.397895 0.4054651 3.098590 2.842970
    ## 28 11 7.587817 0.0000000 2.079442 2.397895 0.3566749 2.586689 3.016934
    ## 29 16 7.588324 0.6931472 2.484907 2.890372 0.6286087 3.157000 2.764746
    ## 30  7 7.588830 0.0000000 2.833213 2.397895 0.6286087 3.401197 2.351375
    ## 31 27 7.589336 0.0000000 2.079442 3.737670 0.6190392 3.429368 3.135494
    ## 32 22 7.589842 0.0000000 3.332205 2.890372 0.4054651 3.218876 3.075775
    ## 33 22 7.590347 0.6931472 3.135494 3.218876 0.3566749 3.213145 2.921624
    ## 34 10 7.590852 0.0000000 3.135494 3.526361 0.0000000 2.148434 3.071370
    ## 35 21 7.591357 0.0000000 2.397895 3.295837 0.6190392 3.189888 2.484907
    ## 36 21 7.591862 0.0000000 3.091042 3.295837 0.4855078 3.203762 2.593387
    ## 37  0 7.592366 0.0000000 3.091042 3.806662 0.5389965 3.381966 2.944439
    ## 38  9 7.592870 0.0000000 0.000000 2.397895 0.5596158 3.277145 2.764746
    ## 39 27 7.593374 0.0000000 2.302585 0.000000 0.5596158 3.518980 2.890372
    ## 40  3 7.593878 0.6931472 3.332205 3.218876 0.6286087 3.392829 2.699682
    ## 41 31 7.594381 0.0000000 1.386294 2.995732 0.4054651 3.113515 2.855032
    ## 42 10 7.594884 0.6931472 3.465736 3.178054 0.4855078 3.442019 2.535679
    ## 43  3 7.595387 0.0000000 2.397895 3.295837 0.4855078 3.305054 2.374906
    ## 44 10 7.595890 0.0000000 1.386294 2.890372 0.5596158 3.238678 2.593387
    ## 45 38 7.596392 0.0000000 2.397895 2.079442 0.6286087 3.555348 2.639057
    ## 46 45 7.596894 0.6931472 3.663562 2.639057 0.6286087 3.537330 2.639057
    ## 47 26 7.597396 0.6931472 3.828641 3.218876 0.5108256 3.178054 3.098590
    ## 48 33 7.597898 0.6931472 3.295837 3.295837 0.6061358 3.713572 3.083438
    ## 49 52 7.598399 0.6931472 3.526361 2.708050 0.6061358 3.888413 2.833213
    ## 50 52 7.598900 0.6931472 3.970292 2.890372 0.6931472 3.768922 3.068053
    ## 51 47 7.599401 0.6931472 3.970292 2.079442 0.6931472 3.975668 2.669210
    ## 52 17 7.599902 0.6931472 3.871201 3.637586 0.6190392 3.754533 2.772589
    ## 53 38 7.600402 0.0000000 2.890372 2.079442 0.6190392 3.483435 2.708050
    ## 54 30 7.600902 0.6931472 3.663562 2.708050 0.6190392 3.699537 3.097515
    ## 55 34 7.601402 0.6931472 3.433987 3.178054 0.6190392 3.764517 3.016934
    ## 56 24 7.601902 0.6931472 3.555348 2.397895 0.6061358 3.806662 2.512306
    ## 57 20 7.602401 0.6931472 3.218876 2.639057 0.4855078 3.362976 3.119055
    ## 58 16 7.602900 0.6931472 3.044522 2.639057 0.4855078 3.526361 2.944439
    ## 59 24 7.603399 0.6931472 2.833213 3.465736 0.4519851 3.587479 3.116685
    ## 60 14 7.603898 0.0000000 3.218876 2.397895 0.5389965 3.500741 2.531313
    ## 61 21 7.604396 0.6931472 2.708050 2.708050 0.6190392 3.391628 2.564949
    ## 62 30 7.604894 0.6931472 3.091042 3.761200 0.5389965 3.723971 3.189888
    ## 63 49 7.605392 0.0000000 3.433987 2.397895 0.6190392 3.761200 2.553900
    ## 64 41 7.605890 0.6931472 3.912023 2.890372 0.6931472 3.591542 2.410542
    ## 65 34 7.606387 0.6931472 3.737670 3.465736 0.4519851 3.279783 2.981126
    ## 66 20 7.606885 0.6931472 3.555348 3.218876 0.4519851 3.327192 2.988708
    ## 67  9 7.607381 0.0000000 3.044522 2.890372 0.6931472 3.438493 2.575661
    ## 68 20 7.607878 0.0000000 2.302585 3.178054 0.4519851 3.097386 2.849550
    ## 69 38 7.608374 0.0000000 3.044522 3.044522 0.4054651 3.390136 3.277145
    ## 70 27 7.608871 0.6931472 3.663562 1.386294 0.6190392 3.483435 2.906120
    ## 71 24 7.609367 0.6931472 3.332205 2.397895 0.6061358 3.444682 2.564949
    ## 72 22 7.609862 0.6931472 3.218876 2.302585 0.4054651 3.205453 3.191847
    ## 73 25 7.610358 0.6931472 3.135494 2.772589 0.5110256 3.282996 3.159393
    ##           x8        x9       x10       x11      x12        x13       x14
    ## 1  0.3600027 0.0000000 0.3364722 3.4339872 1.386294 -27.000000 0.6931472
    ## 2  0.5790339 0.6931472 0.0000000 3.4339872 3.258097  -5.000000 0.4519851
    ## 3  0.5962974 0.0000000 0.3364722 3.4339872 2.639057 -17.000000 0.6061358
    ## 4  0.7490757 0.6931472 0.3364722 3.4339872 3.433987   0.000000 0.3566749
    ## 5  0.7609698 0.0000000 0.3364722 3.4339872 3.433987   0.000000 0.3566749
    ## 6  0.9592568 0.0000000 0.4054651 3.4339872 3.433987   0.000000 0.3566749
    ## 7  1.2878543 0.0000000 0.4054651 3.4339872 3.433987   0.000000 0.5877867
    ## 8  1.0824176 0.0000000 0.5306283 3.4339872 3.433987   0.000000 0.3566749
    ## 9  0.6312718 0.6931472 0.2623643 3.4339872 3.433987   0.000000 0.5389965
    ## 10 0.8023465 0.0000000 0.4054651 3.4339872 3.433987   0.000000 0.4519851
    ## 11 1.2412686 0.6931472 0.3364722 2.6390573 3.433987   2.890372 0.3566749
    ## 12 0.8546916 0.0000000 0.4700036 3.4339872 3.433987   0.000000 0.2513144
    ## 13 0.7793249 0.0000000 0.5306283 2.9957323 3.433987   2.484907 0.2876821
    ## 14 1.0451236 0.0000000 0.4418328 3.4339872 3.433987   0.000000 0.6190392
    ## 15 0.9309969 0.0000000 0.4054651 3.4339872 3.433987   0.000000 0.5389965
    ## 16 0.4989912 0.0000000 0.5877867 3.4339872 3.433987   0.000000 0.3566749
    ## 17 0.9241033 0.6931472 0.3364722 3.4339872 3.433987   0.000000 0.2513144
    ## 18 0.6250937 0.0000000 0.4353181 3.4339872 3.433987   0.000000 0.4519851
    ## 19 1.2798649 0.6931472 0.5108256 3.4339872 3.433987   0.000000 0.4519851
    ## 20 0.9352872 0.0000000 0.5306283 3.4339872 3.433987   0.000000 0.5389965
    ## 21 1.2377251 0.6931472 0.5306283 2.0794415 3.433987   3.178054 0.6190392
    ## 22 0.8672832 0.0000000 0.5753641 3.4339872 3.433987   0.000000 0.5389965
    ## 23 0.7717095 0.0000000 0.4700036 3.4339872 2.302585 -21.000000 0.5389965
    ## 24 0.9725716 0.0000000 0.4700036 2.6390573 2.833213   1.386294 0.6190392
    ## 25 0.5703398 0.0000000 0.5877867 3.4339872 3.433987   0.000000 0.4519851
    ## 26 0.3480252 0.0000000 0.4924765 3.4339872 2.079442 -23.000000 0.6931472
    ## 27 0.8369351 0.0000000 0.3101549 3.4339872 3.433987   0.000000 0.4855078
    ## 28 0.4900225 0.6931472 0.3746934 3.4339872 3.433987   0.000000 0.4054651
    ## 29 0.9213201 0.6931472 0.4924765 1.9459101 3.433987   3.218876 0.4855078
    ## 30 1.3993664 0.6931472 0.5465437 2.4849066 3.433987   2.995732 0.5596158
    ## 31 0.8574502 0.6931472 0.5978370 2.3978953 2.079442  -3.000000 0.6286087
    ## 32 0.7707054 0.0000000 0.5465437 3.4339872 3.433987   0.000000 0.4855078
    ## 33 0.8576966 0.6931472 0.4353181 3.4339872 2.484907 -19.000000 0.6286087
    ## 34 0.3133904 0.0000000 0.3101549 3.4339872 3.258097  -5.000000 0.4855078
    ## 35 1.1368335 0.6931472 0.0000000 3.0445224 1.098612 -18.000000 0.6931472
    ## 36 1.0678406 0.0000000 0.4924765 3.4339872 1.609438 -26.000000 0.6286087
    ## 37 0.9475433 0.6931472 0.4924765 3.0445224 1.386294 -17.000000 0.6931472
    ## 38 0.9985288 0.0000000 0.5465437 2.3025851 1.609438  -5.000000 0.6286087
    ## 39 1.0737971 0.6931472 0.5465437 2.3978953 2.197225  -2.000000 0.6286087
    ## 40 1.1223523 0.6931472 0.5978370 0.6931472 2.890372   2.833213 0.5596158
    ## 41 0.8385355 0.6931472 0.5978370 3.4339872 2.995732 -11.000000 0.5596158
    ## 42 1.2815310 0.0000000 0.4353181 2.8903718 2.397895  -7.000000 0.5596158
    ## 43 1.3062517 0.0000000 0.4353181 0.0000000 2.995732   2.995732 0.5596158
    ## 44 1.0918555 0.0000000 0.4353181 3.0445224 3.433987   2.397895 0.4855078
    ## 45 1.2851982 0.6931472 0.4924765 2.3978953 3.433987   3.044522 0.4054651
    ## 46 1.2718112 0.6931472 0.5978370 1.9459101 3.178054   2.890372 0.5596158
    ## 47 0.7355427 0.6931472 0.6466272 3.0445224 2.079442 -13.000000 0.6286087
    ## 48 1.0715836 0.0000000 0.5108256 2.3978953 3.433987   3.044522 0.4054651
    ## 49 1.3836868 0.0000000 0.6061358 1.7917595 3.433987   3.258097 0.4855078
    ## 50 1.1200608 0.6931472 0.6061358 1.3862944 3.433987   3.332205 0.4855078
    ## 51 1.5879317 0.6931472 0.6931472 0.6931472 3.433987   3.401197 0.3566749
    ## 52 1.3299759 0.6931472 0.6505876 1.9459101 2.708050   2.197225 0.6190392
    ## 53 1.1802236 0.6931472 0.5978370 1.9459101 2.484907   1.791759 0.6190392
    ## 54 1.0525212 0.6931472 0.5978370 1.7917595 2.397895   1.791759 0.6190392
    ## 55 1.1534532 0.6931472 0.5596158 2.1972246 2.639057   1.791759 0.6190392
    ## 56 1.5856273 0.0000000 0.6061358 1.9459101 2.772589   2.302585 0.5389965
    ## 57 0.8281226 0.6931472 0.5978370 3.4339872 1.791759 -25.000000 0.6931472
    ## 58 1.0414539 0.6931472 0.5108256 3.1780538 1.609438 -19.000000 0.6286087
    ## 59 0.9666564 0.0000000 0.5108256 3.4339872 2.397895 -20.000000 0.6190392
    ## 60 1.3291614 0.0000000 0.4595323 2.8332133 1.609438 -12.000000 0.6931472
    ## 61 1.2216724 0.0000000 0.5465437 2.3025851 3.433987   3.091042 0.5596158
    ## 62 1.0065688 0.6931472 0.6539265 2.4849066 2.944439   2.079442 0.5596158
    ## 63 1.5134046 0.6931472 0.5596158 2.1972246 1.945910  -2.000000 0.6286087
    ## 64 1.4996847 0.6931472 0.6539265 0.6931472 3.433987   3.401197 0.4519851
    ## 65 0.8614750 0.0000000 0.6539265 3.4339872 3.433987   0.000000 0.4054651
    ## 66 0.8854926 0.0000000 0.4595323 3.4339872 3.135494  -8.000000 0.5389965
    ## 67 1.2478084 0.6931472 0.4307829 1.3862944 2.564949   2.302585 0.6190392
    ## 68 0.8322680 0.0000000 0.6131045 3.4339872 3.433987   0.000000 0.4519851
    ## 69 0.7534488 0.0000000 0.2876821 3.4339872 2.302585 -21.000000 0.6190392
    ## 70 1.0390202 0.0000000 0.4307829 2.4849066 3.433987   2.995732 0.5389965
    ## 71 1.2606681 0.6931472 0.5389965 2.7080502 3.433987   2.833213 0.4519851
    ## 72 0.7002646 0.0000000 0.5260931 3.4339872 1.386294 -27.000000 0.6931472
    ## 73 0.6960779 0.0000000 0.5203046 3.3672958 2.197225 -20.000000 0.6190393
    ##          x15       x16       x17       x18      x19      x20      x21
    ## 1  0.5877867 0.6931472 0.6931472 0.0000000 4.418841 4.174387 4.301359
    ## 2  0.6931472 0.6931472 0.6931472 0.0000000 4.342506 4.094345 4.210645
    ## 3  0.4924765 0.6931472 0.6931472 0.0000000 4.465908 4.262680 4.262680
    ## 4  0.6418539 0.0000000 0.0000000 0.0000000 4.249923 3.850148 4.041295
    ## 5  0.3746934 0.0000000 0.0000000 0.0000000 4.290459 4.075841 4.191169
    ## 6  0.4353181 0.0000000 0.0000000 0.0000000 4.234107 3.990834 4.115780
    ## 7  0.4054651 0.6931472 0.0000000 0.0000000 4.356709 4.112512 4.220977
    ## 8  0.4924765 0.0000000 0.0000000 0.6931472 4.112512 3.808882 3.955082
    ## 9  0.2411621 0.0000000 0.0000000 0.0000000 4.144721 3.850148 3.987130
    ## 10 0.4700036 0.0000000 0.0000000 0.0000000 4.249923 3.784190 4.018183
    ## 11 0.3364722 0.6931472 0.0000000 0.0000000 4.204693 3.637586 3.925926
    ## 12 0.2623643 0.0000000 0.0000000 0.0000000 4.342506 4.041295 4.169761
    ## 13 0.2623643 0.0000000 0.0000000 0.6931472 4.370713 4.060443 4.194190
    ## 14 0.3364722 0.6931472 0.6931472 0.0000000 4.330733 3.784190 4.037774
    ## 15 0.5306283 0.6931472 0.0000000 0.6931472 4.395683 4.007333 4.215086
    ## 16 0.4700036 0.0000000 0.0000000 0.0000000 4.316154 4.007333 4.125520
    ## 17 0.2623643 0.0000000 0.0000000 0.6931472 4.158883 3.660994 3.941582
    ## 18 0.2623643 0.0000000 0.0000000 0.0000000 4.316154 4.025352 4.143135
    ## 19 0.3364722 0.6931472 0.6931472 0.0000000 4.406719 4.041295 4.220977
    ## 20 0.4700036 0.6931472 0.0000000 0.0000000 4.356709 4.158883 4.241327
    ## 21 0.4700036 0.6931472 0.6931472 0.6931472 4.249923 3.784190 3.988984
    ## 22 0.6418539 0.0000000 0.6931472 0.6931472 4.342506 3.931826 4.127134
    ## 23 0.5306283 0.0000000 0.6931472 0.0000000 4.204693 3.990834 4.069027
    ## 24 0.5877867 0.0000000 0.6931472 0.6931472 4.330733 3.970292 4.127134
    ## 25 0.4700036 0.6931472 0.0000000 0.0000000 4.316154 3.828641 4.084294
    ## 26 0.4054651 0.6931472 0.6931472 0.0000000 4.395683 4.094345 4.234107
    ## 27 0.6466272 0.0000000 0.0000000 0.0000000 4.395683 4.158883 4.251348
    ## 28 0.4353181 0.6931472 0.6931472 0.6931472 4.453184 3.949319 4.091006
    ## 29 0.4353181 0.0000000 0.6931472 0.6931472 4.174387 3.990834 4.070735
    ## 30 0.4353181 0.6931472 0.6931472 0.6931472 4.382027 4.249923 4.294561
    ## 31 0.5978370 0.6931472 0.6931472 0.0000000 4.234107 3.637586 3.937691
    ## 32 0.6466272 0.6931472 0.0000000 0.6931472 4.440296 4.234107 4.290459
    ## 33 0.3746934 0.6931472 0.6931472 0.0000000 4.382027 4.023564 4.157319
    ## 34 0.5978370 0.0000000 0.0000000 0.0000000 4.429626 4.188138 4.295924
    ## 35 0.4924765 0.6931472 0.6931472 0.6931472 4.342506 3.711130 4.107590
    ## 36 0.6931472 0.6931472 0.6931472 0.6931472 4.302713 3.784190 4.051785
    ## 37 0.6466272 0.6931472 0.6931472 0.6931472 4.158883 3.711130 3.923952
    ## 38 0.6931472 0.6931472 0.6931472 0.6931472 4.382027 4.172848 4.213608
    ## 39 0.5978370 0.6931472 0.6931472 0.0000000 4.342506 3.931826 4.160444
    ## 40 0.4924765 0.6931472 0.6931472 0.0000000 4.418841 3.953165 4.185099
    ## 41 0.4924765 0.6931472 0.6931472 0.0000000 4.488636 4.218036 4.339902
    ## 42 0.5465437 0.6931472 0.6931472 0.6931472 4.330733 3.891820 4.120662
    ## 43 0.5465437 0.6931472 0.6931472 0.6931472 4.418841 4.204693 4.305416
    ## 44 0.5465437 0.6931472 0.0000000 0.6931472 4.442651 3.869116 4.162003
    ## 45 0.4353181 0.0000000 0.0000000 0.0000000 4.442651 4.060443 4.216562
    ## 46 0.4353181 0.6931472 0.6931472 0.0000000 4.290459 3.828641 3.887730
    ## 47 0.5465437 0.6931472 0.0000000 0.6931472 4.442651 4.141546 4.290459
    ## 48 0.5978370 0.6931472 0.6931472 0.0000000 4.382027 4.201703 4.279440
    ## 49 0.3746934 0.6931472 0.0000000 0.6931472 4.418841 4.172848 4.252772
    ## 50 0.4353181 0.6931472 0.6931472 0.6931472 4.356709 3.953165 4.105944
    ## 51 0.4353181 0.0000000 0.0000000 0.6931472 4.465908 4.174387 4.308111
    ## 52 0.3746934 0.6931472 0.0000000 0.6931472 4.418841 4.204693 4.300003
    ## 53 0.5978370 0.6931472 0.6931472 0.6931472 4.478473 4.174387 4.326778
    ## 54 0.5465437 0.6931472 0.0000000 0.6931472 4.382027 4.174387 4.232656
    ## 55 0.4924765 0.6931472 0.6931472 0.6931472 4.423648 4.032469 4.232656
    ## 56 0.4924765 0.6931472 0.6931472 0.6931472 4.342506 4.128746 4.107590
    ## 57 0.5465437 0.6931472 0.6931472 0.6931472 4.342506 4.007333 4.163560
    ## 58 0.6505876 0.6931472 0.6931472 0.6931472 4.406719 4.249923 4.318821
    ## 59 0.6466272 0.6931472 0.6931472 0.6931472 4.423648 4.234107 4.325456
    ## 60 0.6061358 0.6931472 0.6931472 0.6931472 4.234107 4.063885 4.160444
    ## 61 0.6061358 0.6931472 0.6931472 0.6931472 4.382027 4.094345 4.255613
    ## 62 0.5108256 0.6931472 0.6931472 0.6931472 4.234107 4.152613 4.188138
    ## 63 0.6061358 0.6931472 0.6931472 0.6931472 4.262680 4.067316 4.158883
    ## 64 0.5596158 0.6931472 0.6931472 0.6931472 4.465908 4.259859 4.360548
    ## 65 0.4595323 0.6931472 0.6931472 0.6931472 4.406719 4.075841 4.262680
    ## 66 0.4054651 0.6931472 0.6931472 0.6931472 4.316154 4.174387 4.245634
    ## 67 0.5389965 0.6931472 0.6931472 0.6931472 4.316154 4.218036 4.275276
    ## 68 0.6190392 0.0000000 0.0000000 0.6931472 4.453184 4.204693 4.279440
    ## 69 0.4795731 0.6931472 0.6931472 0.6931472 4.356709 3.891820 4.065602
    ## 70 0.5705449 0.6931472 0.6931472 0.6931472 4.442651 4.204693 4.330733
    ## 71 0.5705449 0.0000000 0.6931472 0.6931472 4.418841 4.290459 4.330733
    ## 72 0.4795731 0.6931472 0.6931472 0.6931472 4.406719 4.143135 4.343805
    ## 73 0.5604040 0.6931472 0.0000000 0.6931472 4.394449 4.110874 4.330733
    ##            x22   x23
    ## 1  0.000000000  1.96
    ## 2  0.000000000 11.39
    ## 3  0.000000000 10.13
    ## 4  0.000000000  5.52
    ## 5  0.000000000  6.90
    ## 6  0.000000000 11.39
    ## 7  0.000000000  8.29
    ## 8  0.000000000 12.54
    ## 9  0.000000000 10.59
    ## 10 0.000000000  4.83
    ## 11 0.000000000  7.60
    ## 12 0.000000000 11.74
    ## 13 0.000000000  4.72
    ## 14 0.000000000 15.54
    ## 15 0.000000000  3.57
    ## 16 0.000000000  3.34
    ## 17 0.000000000  8.75
    ## 18 0.000000000  1.87
    ## 19 0.000000000  2.30
    ## 20 0.000000000  9.44
    ## 21 0.000000000  9.32
    ## 22 0.000000000  9.44
    ## 23 0.000000000  7.83
    ## 24 0.000000000  7.25
    ## 25 0.000000000  6.90
    ## 26 0.000000000  6.79
    ## 27 0.000000000  6.79
    ## 28 0.000000000 16.92
    ## 29 0.039220713 10.36
    ## 30 0.000000000  3.57
    ## 31 0.000000000  9.44
    ## 32 0.357674444  4.26
    ## 33 0.000000000  3.91
    ## 34 0.000000000  8.29
    ## 35 0.000000000  3.80
    ## 36 0.000000000  7.48
    ## 37 0.000000000 14.61
    ## 38 0.182321557  6.33
    ## 39 0.000000000  7.83
    ## 40 0.000000000  4.14
    ## 41 0.000000000  2.42
    ## 42 0.000000000  8.63
    ## 43 0.113328685 12.33
    ## 44 0.000000000  2.99
    ## 45 0.974559640 14.73
    ## 46 0.329303747 15.31
    ## 47 0.000000000  3.80
    ## 48 0.916290732  8.29
    ## 49 0.000000000  8.75
    ## 50 0.536493371 11.16
    ## 51 0.000000000 11.97
    ## 52 0.488580015  9.44
    ## 53 0.000000000  7.02
    ## 54 0.000000000 10.59
    ## 55 0.000000000  1.96
    ## 56 0.000000000 13.69
    ## 57 0.000000000  9.44
    ## 58 0.000000000 14.38
    ## 59 0.009950331  4.14
    ## 60 0.000000000 13.92
    ## 61 0.751416089 12.43
    ## 62 0.122217633 11.16
    ## 63 0.039220713 10.70
    ## 64 0.000000000  5.52
    ## 65 0.000000000  5.64
    ## 66 0.774727168 10.24
    ## 67 0.009950331 20.48
    ## 68 0.970778917  8.29
    ## 69 0.000000000 14.15
    ## 70 0.000000000  4.00
    ## 71 0.000000000  8.00
    ## 72 0.019802627  8.00
    ## 73 0.009950331  7.00

``` r
flogeorge_log.lm <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22,data=flogeorge_log)
flogeorge_log.lm
```

    ## 
    ## Call:
    ## lm(formula = y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + 
    ##     x13 + x22, data = flogeorge_log)
    ## 
    ## Coefficients:
    ## (Intercept)           x1           x3           x4           x5  
    ##  -1428.9628     184.6511       3.5399      -3.9553      -6.2654  
    ##          x6           x8           x9          x10          x11  
    ##     15.5035      -8.6423       4.4820      10.2240       1.4594  
    ##         x13          x22  
    ##      0.2044       4.2607

### using anova to check the significance of regression for the two models with null hypothesis being loglinear model is better and alternate hypothesis being linear model is better

``` r
anova(flogeorge_log.lm,flogeorge.lm)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22
    ## Model 2: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + 
    ##     x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + 
    ##     x22 + x23
    ##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  
    ## 1     60 6202.1                              
    ## 2     49 3995.3 11    2206.8 2.4605 0.01544 *
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### given the pvalue is low (0.01544) for a significance value of 0.05, we can reject the null hypothesis hence linear model is more regressionally significant than the loglinear model. Hence we prefer the linear model

``` r
library(modelr)
```

    ## Warning: package 'modelr' was built under R version 3.5.3

``` r
rmse(flogeorge_log.lm,data = flogeorge_log)
```

    ## [1] 9.281189

``` r
rmse(flogeorge.lm1,data=flogeorge)
```

    ## [1] 8.558107

### also rmse for the linear model is lesser than log linear

### 2.b Some people claim that the Florida’s number of wins/ total games in the previous season (X10) has a quadratic relationship with the dependent variable. In your model add a squared X10 variable and test if your model improves. Show your results. Would you add this variable to your model?

``` r
flogeorge$x10 <- (flogeorge[1:73,11])
flogeorge$x10_2 <- (flogeorge[1:73,11])^2
flogeorge_x10_sq.lm <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+x10_2,data=flogeorge)
rmse(flogeorge_x10_sq.lm,data = flogeorge)
```

    ## [1] 8.322092

``` r
rmse(flogeorge.lm1,data=flogeorge)
```

    ## [1] 8.558107

### as you can see the RMSE value for the squared x10 linear model is lesser than the regular linear model. Hence we can add this variable to our model

3. A researcher claims that starting in 1990 the average number of points scored by Florida increased. Accordingly, we want to test if there is a structural break starting in this year.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

``` r
library(ggplot2)
```

    ## Warning: package 'ggplot2' was built under R version 3.5.3

``` r
ggplot(flogeorge,aes(x1,y))+geom_point(colour="blue")
```

![](final_assgn_files/figure-markdown_github/unnamed-chunk-15-1.png)

### 3.a Create a dummy variable for a change in the intercept, run the regression and explain your results. If there is evidence of a structural change, starting in 1990, then leave this new explanatory variable in your model.

### The null and alternative hypothesis is as follows

Null Hypothesis: No structural break in the dataset Alternative Hypothesis: Structural break in the dataset

``` r
library(strucchange)
```

    ## Warning: package 'strucchange' was built under R version 3.5.3

    ## Loading required package: zoo

    ## Warning: package 'zoo' was built under R version 3.5.3

    ## 
    ## Attaching package: 'zoo'

    ## The following objects are masked from 'package:base':
    ## 
    ##     as.Date, as.Date.numeric

    ## Loading required package: sandwich

    ## Warning: package 'sandwich' was built under R version 3.5.3

``` r
model_struc_change <- Fstats(flogeorge$y~1,from=0.01)
```

    ## Warning in Fstats(flogeorge$y ~ 1, from = 0.01): 'from' changed (was too
    ## small)

    ## Warning in Fstats(flogeorge$y ~ 1, from = 0.01): 'to' changed (was too
    ## large)

``` r
sctest(model_struc_change)
```

    ## 
    ##  supF test
    ## 
    ## data:  model_struc_change
    ## sup.F = 40.003, p-value = 8.171e-09

``` r
strucchange::breakpoints(flogeorge$y~1)
```

    ## 
    ##   Optimal 3-segment partition: 
    ## 
    ## Call:
    ## breakpoints.formula(formula = flogeorge$y ~ 1)
    ## 
    ## Breakpoints at observation number:
    ## 44 55 
    ## 
    ## Corresponding to breakdates:
    ## 0.6027397 0.7534247

### clearly the pvalue is extremely low (8.171e-09) for any significance value hence due to low pvalue we can reject the null hypothesis and conclude that there is a structural break in the dataset. And the breakpoints occur at 1990 and 2001. Hence we can also say structural break exists at 1990.

### Hence assuming a dummy variable β25 where β25=0 when there is no structural change in that year while it is equal to 1 when there is a structural change.

``` r
flogeorge$sc <- 0
flogeorge$sc[45] <- 1
flogeorge$sc[56] <- 1
flogeorge_strucch <- lm(y~x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc, data = flogeorge)
coef(flogeorge_strucch)[1] + coef(flogeorge_strucch)[21] # intercept value when structural change exists
```

    ## (Intercept) 
    ##   -466.1874

``` r
coef(flogeorge_strucch)[1] #when no structural change exists
```

    ## (Intercept) 
    ##    -464.665

### from the above intercept values we can see a change in the intercept values because of our structural change dummy variable.

4. As we know, there were some excellent coaches in each team that took them to national championships. Many people will complain that our model does not capture the effect of these great coaches. Looking in the history of each team I found the following coaches that were in charge of these teams for more than five years:
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Georgia Wally Butts 1946-1960 Vince Dooley 1964-1988 Ray Goff 1989-1995 Mark Richt 2001- 2015

Florida Bob Woodruff 1950-1959 Roy Graves 1960-1969 Doug Dickey 1970-1978 Steve Spurrier 1990-2001 Urban Meyer 2005- 2010

### 4.a Explain how do you plan to create variables that capture the effect on these teams for each of these coaches? Create those variables (assume that the effect of each coach will change the intercept but not the slope of the regression function).

### Ans - we create 4+5 dummy variables which takes the value 1 when that particular person is the coach of the said team and otherwise is equals to 0

``` r
flogeorge$wally <- 0
flogeorge$wally[1:15] <- 1
flogeorge$vince <- 0
flogeorge$vince[19:43] <- 1
flogeorge$ray <- 0
flogeorge$ray[44:50] <- 1
flogeorge$mark <- 0
flogeorge$mark[56:70] <- 1
flogeorge$bob <- 0
flogeorge$bob[5:14] <- 1
flogeorge$roy <- 0
flogeorge$roy[15:24] <- 1
flogeorge$doug <- 0
flogeorge$doug[25:33] <- 1
flogeorge$steve <- 0
flogeorge$steve[45:56] <- 1
flogeorge$urban <- 0
flogeorge$urban[60:65] <- 1
```

### 4.b Introduce each of these variables, one at the time, and use the procedure test for the incremental of marginal contribution (use a significance level of 10%) of an explanatory variable for each of these new variables. Which variables are good to include?

### procedure - we compare our linear model with the coach variable added, with our baseline linear model without coaches using anova. Null hypothesis in this case is linear model without coach influence is better while alternate hypothesis is that the coach variable has a regresiion influence in our model. Since given significance value is 0.1, if our pvalue is lesser than 0.1 we reject the null hypothesis

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22 + x10_2 + sc + wally, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + wally
    ##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  
    ## 1     51 3782.3                              
    ## 2     57 4974.6 -6   -1192.3 2.6796 0.02449 *
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### here, the pvalue is 0.02449 which is lesser than 0.1 hence we reject the null hypothesis i.e coach wally's tenure is regressionally significant

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + wally + vince, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + wally + vince
    ##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  
    ## 1     51 3782.3                              
    ## 2     56 4727.3 -5   -945.06 2.5486 0.03915 *
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### in this case, the addition of vince increased the pvalue to 0.039 which is less than 0.1 hence we reject the null hypothesis and conclude coach vince tenure is regressionally significant

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + wally + vince + ray, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + wally + vince + ray
    ##   Res.Df    RSS Df Sum of Sq      F Pr(>F)
    ## 1     51 3782.3                           
    ## 2     55 4383.1 -4   -600.86 2.0255 0.1047

### addition of ray caused the pvalue to go up greater than 0.1 hence regressionally insignificant

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + wally + vince + ray + mark, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + wally + vince + ray + mark
    ##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  
    ## 1     51 3782.3                              
    ## 2     54 4378.6 -3   -596.33 2.6803 0.05658 .
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### addition of mark caused the pvalue to go down to 0.0568 lesser than 0.1 hence regressionally significant

### hence we can clearly say that out of all the florida coaches only Coach Mark's tenure proved to be rgressionally significant

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + bob, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + bob
    ##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  
    ## 1     51 3782.3                              
    ## 2     57 4971.9 -6   -1189.6 2.6735 0.02476 *
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### coach bob is regresionally significant since pvalue &lt; 0.1

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + bob + roy, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + bob + roy
    ##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  
    ## 1     51 3782.3                              
    ## 2     56 4962.3 -5     -1180 3.1821 0.01414 *
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### addition of coach roy brought down the pvalue t0 0.01 and is regressionally significant

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + bob + roy + doug, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + bob + roy + doug
    ##   Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
    ## 1     51 3782.3                                
    ## 2     55 4957.5 -4   -1175.2 3.9616 0.007103 **
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### addition of doug caused a dramatic decrease in the pvalue hence regressionally siginficant

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + bob + roy + doug + steve, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + bob + roy + doug + steve
    ##   Res.Df    RSS Df Sum of Sq     F  Pr(>F)  
    ## 1     51 3782.3                             
    ## 2     54 4673.6 -3   -891.28 4.006 0.01231 *
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### addition of steve increased the pvalue but on the whole is still regressionally significant

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + bob + roy + doug + steve + urban, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + bob + roy + doug + steve + urban
    ##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  
    ## 1     51 3782.3                              
    ## 2     53 4262.8 -2   -480.53 3.2397 0.04737 *
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### addition of urban increased the pvalue but still the current model is regressionally significant

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + bob + roy + doug, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + bob + roy + doug
    ##   Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
    ## 1     51 3782.3                                
    ## 2     55 4957.5 -4   -1175.2 3.9616 0.007103 **
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### from this model, we can see that addition of only coaches bob roy and doug makes the entire model more regressionally significant hence out of all Georgia coaches, Coach Doug and then Coach Bob's and Roy's tenure have been regressionally significant.

### 4.c Write down and show the results for the final model you selected with the coaches that made an impact on the Florida-Georgia game.

``` r
fg_coach <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc + doug + mark, data = flogeorge)
anova(flogeorge_strucch,fg_coach)
```

    ## Analysis of Variance Table
    ## 
    ## Model 1: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x12 + x14 + 
    ##     x15 + x16 + x17 + x18 + x21 + x22 + x23 + x10_2 + sc
    ## Model 2: y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + x13 + x22 + 
    ##     x10_2 + sc + doug + mark
    ##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  
    ## 1     51 3782.3                              
    ## 2     56 4949.3 -5     -1167 3.1473 0.01496 *
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

``` r
rmse(flogeorge_strucch,data = flogeorge)
```

    ## [1] 7.247872

``` r
rmse(fg_coach,data = flogeorge)
```

    ## [1] 8.290999

### We can clearly see that the addition of coach variables caused a significant increase in the rmse values and from the above anova results, but if we do not consider rmse values then we have chosen Coaches mark and doug

### 4.d If you are a Florida fan and you can pick any coach from this period, which one you would like for your confrontation with Georgia? Explain why?. If you are a Georgia fan and you can pick any coach from this period, which one you would like for your confrontation with Florida?Explain why?

### As mentioned before as a Florida fan i would choose Coach Mark while as a Georgia fan i would choose Coach Doug purely based on the ANOVA results

5.
--

### a. Show the final regression model explaining the number of points Florida scored against Georgia. What do you think of the goodness of fit of your model? (Explain)

``` r
pred1 <- flogeorge[1:73,2:35]
fg_final <- lm(y~x1+x3+x4+x5+x6+x8+x9+x10+x11+x13+x22+ x10_2 + sc, data = flogeorge)
results <- floor(predict(fg_coach,newdata = pred1))
for (i in 1:73){
  if(results[i]<0 || is.na(results[i]))
    results[i] = 0
}
results
```

    ##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 
    ##  0 15  8 21 18 13 21 24 15 10 14 20 15  9  7 14 19 12 18 16 12 19  0 19 20 
    ## 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 
    ##  0 17 16 13 20 12 25 17  8 15 11 11 13 27 19 22 15  7 15 31 30 27 30 38 41 
    ## 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 
    ## 51 32 27 35 31 30 29 26 21 18 21 30 29 31 29 25 15 25 22 29 30 26 25

### goodness of fit (using Kolmogorov-Smirnov test)

H0 = The data is consistent with the linear model. H1 = The data is NOT consistent with the linear model

``` r
ks.test(results[2:73],flogeorge$y[2:73])
```

    ## Warning in ks.test(results[2:73], flogeorge$y[2:73]): cannot compute exact
    ## p-value with ties

    ## 
    ##  Two-sample Kolmogorov-Smirnov test
    ## 
    ## data:  results[2:73] and flogeorge$y[2:73]
    ## D = 0.15278, p-value = 0.3701
    ## alternative hypothesis: two-sided

since pvalue is very high we cannot reject the null hypothesis hence the linear model is consistent with our data.
==================================================================================================================

### 5.b Given the following information for 2019, predict the number of points Florida is going to score in the upcoming match (use only the information that is relevant for your final model)

X1=2019,X2=1,X3=25,X4=,X5=0.735,X6=26.0866,X7=20.5863,X8=1.05112,X9=0,X10=0.65988,X11=24,X12=12,X13=12,X14=0.682155,X15=0.72586,X16=1,X17=,X18=,X19=8,X20=62,X21=77,X22=0,X23=8

``` r
fg_final
```

    ## 
    ## Call:
    ## lm(formula = y ~ x1 + x3 + x4 + x5 + x6 + x8 + x9 + x10 + x11 + 
    ##     x13 + x22 + x10_2 + sc, data = flogeorge)
    ## 
    ## Coefficients:
    ## (Intercept)           x1           x3           x4           x5  
    ##  -132.64732      0.07137      0.15111     -0.34912     -5.71077  
    ##          x6           x8           x9          x10          x11  
    ##     0.80858     -3.25510      2.99717    -34.92812      0.34537  
    ##         x13          x22        x10_2           sc  
    ##     0.17621      2.28757     38.52679      0.57578

``` r
data2019 <- data.frame(x1=2019,x2=1,x3=25,x4=0,x5=0.735,x6=26.0866,x8=1.05112,x9=0,x10=0.65988,x11=24,x12=12,x13=12,x14=0.682155,x15=0.72586,x16=1,x21=77,x22=0,x23=8,x10_2=0.4354416,sc=0,steve=0,vince=0)
floor(predict(fg_final,newdata = data2019))
```

    ##  1 
    ## 32

My model predicts Florida is going to score 32 points
-----------------------------------------------------

### 5.c Create a 95% confidence interval for this predicted score.

``` r
predict(fg_final,newdata = data2019,interval = "confidence",level = 0.95)
```

    ##        fit     lwr      upr
    ## 1 32.83114 22.4182 43.24408
